{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "Load papers and do some preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from redis.asyncio import Redis\n",
    "from utils.embeddings import Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the redis instance running in your docker stack at redis:6379\n",
    "redis_conn = await Redis(host='redis', port='6379')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load papers dataframe\n",
    "def read_paper_df() -> pd.DataFrame:\n",
    "    with open(\"arxiv_papers_df.pkl\", \"rb\") as f:\n",
    "        df = pickle.load(f)\n",
    "    return df\n",
    "\n",
    "def paper_key(paper_id: str) -> str:\n",
    "    return f'paper:{paper_id}'\n",
    "\n",
    "# Function to concurrently load papers into Redis\n",
    "async def gather_with_concurrency(n, redis_conn, *papers):\n",
    "    semaphore = asyncio.Semaphore(n)\n",
    "    async def load_paper(paper):\n",
    "        async with semaphore:\n",
    "            paper['vector'] = np.array(paper['vector'], dtype=np.float32).tobytes()\n",
    "            await redis_conn.hset(paper_key(paper['id']), mapping=paper)\n",
    "    # gather with concurrency\n",
    "    await asyncio.gather(*[load_paper(p) for p in papers])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate\n",
    "df = read_paper_df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['input'] = df.apply(lambda r: r.title + r.abstract, axis=1)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this demo we will take a small sample\n",
    "df = df.sample(frac=0.1)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Creation\n",
    "\n",
    "To create embeddings/vector representations of the papers, we will use a combination of the paper abstract and title fields and pass through an open source `SentenceTransformer` model (after some light preprocessing).\n",
    "\n",
    "Everything is wrapped into the `Embeddings` class and `gather_with_concurrency` function below to help make this cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Embeddings\n",
    "embeddings = Embeddings()\n",
    "vectors = embeddings.make(df.input.to_list(), show_progress=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vector'] = vectors.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to a dict\n",
    "papers = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load papers to Redis\n",
    "await gather_with_concurrency(50, redis_conn, *papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many items were stored\n",
    "await redis_conn.dbsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check a paper\n",
    "key = paper_key(df.sample(1)['id'].iloc[0])\n",
    "await redis_conn.hgetall(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RediSearch Index Creation\n",
    "\n",
    "Now time to create the search index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redis.commands.search.field import TagField\n",
    "from utils.search_index import SearchIndex\n",
    "\n",
    "search_index = SearchIndex('papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_field = TagField(\"categories\")\n",
    "year_field = TagField(\"year\")\n",
    "\n",
    "await search_index.create_flat(\n",
    "    categories_field,\n",
    "    year_field,\n",
    "    redis_conn=redis_conn,\n",
    "    number_of_vectors=len(papers),\n",
    "    prefix=\"paper:\",\n",
    "    distance_metric=\"IP\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1c5a7c9cc0d58080444e081b74a0823c09a12f0209aca730c38726ea6940124"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
