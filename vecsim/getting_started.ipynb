{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "Load papers and do some preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from redis.asyncio import Redis\n",
    "from utils.embeddings import Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the redis instance running in your docker stack at redis:6379\n",
    "redis_conn = await Redis(host=\"redis\", port=\"6379\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load papers dataframe\n",
    "def read_paper_df() -> pd.DataFrame:\n",
    "    with open(\"arxiv_papers_df.pkl\", \"rb\") as f:\n",
    "        df = pickle.load(f)\n",
    "    return df\n",
    "\n",
    "def paper_key(paper_id: str) -> str:\n",
    "    return f\"paper:{paper_id}\"\n",
    "\n",
    "# Function to concurrently load papers into Redis\n",
    "async def gather_with_concurrency(n, redis_conn, *papers):\n",
    "    semaphore = asyncio.Semaphore(n)\n",
    "    async def load_paper(paper):\n",
    "        async with semaphore:\n",
    "            paper[\"vector\"] = np.array(paper[\"vector\"], dtype=np.float32).tobytes()\n",
    "            await redis_conn.hset(paper_key(paper[\"id\"]), mapping=paper)\n",
    "    # gather with concurrency\n",
    "    await asyncio.gather(*[load_paper(p) for p in papers])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate\n",
    "df = read_paper_df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"input\"] = df.apply(lambda r: r.title + r.abstract, axis=1)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this demo we will take a small sample\n",
    "df = df.sample(frac=0.1)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Creation\n",
    "\n",
    "To create embeddings/vector representations of the papers, we will use a combination of the paper abstract and title fields and pass through an open source `SentenceTransformer` model (after some light preprocessing).\n",
    "\n",
    "Everything is wrapped into the `Embeddings` class and `gather_with_concurrency` function below to help make this cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Embeddings\n",
    "embeddings = Embeddings()\n",
    "vectors = embeddings.make(df.input.to_list(), show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"vector\"] = vectors.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert dataframe to a dict\n",
    "papers = df.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load papers to Redis\n",
    "await gather_with_concurrency(50, redis_conn, *papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check how many items were stored\n",
    "await redis_conn.dbsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check a paper\n",
    "key = paper_key(df.sample(1)[\"id\"].iloc[0])\n",
    "await redis_conn.hgetall(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RediSearch Index Creation\n",
    "\n",
    "Now time to create the search index over all of the documents we have now stored in Redis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from redis.commands.search.field import (\n",
    "    TagField,\n",
    "    VectorField,\n",
    "    NumericField,\n",
    "    TextField\n",
    ")\n",
    "from utils.search_index import SearchIndex\n",
    "\n",
    "# Search index helper class\n",
    "search_index = SearchIndex(\"papers\", redis_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a document schema that includes 3 indexed fields\n",
    "# --> vector, categories, and year\n",
    "\n",
    "# vector_field = VectorField(\n",
    "#     \"vector\",\n",
    "#     \"HNSW\", {\n",
    "#         \"TYPE\": \"FLOAT32\",\n",
    "#         \"DIM\": 768,\n",
    "#         \"DISTANCE_METRIC\": \"COSINE\",\n",
    "#         \"INITIAL_CAP\": len(papers),\n",
    "#     }\n",
    "# )\n",
    "\n",
    "vector_field = VectorField(\n",
    "    \"vector\",\n",
    "    \"FLAT\", {\n",
    "        \"TYPE\": \"FLOAT32\",\n",
    "        \"DIM\": 768,\n",
    "        \"DISTANCE_METRIC\": \"IP\",\n",
    "        \"INITIAL_CAP\": len(papers),\n",
    "        \"BLOCK_SIZE\": len(papers)\n",
    "    }\n",
    ")\n",
    "categories_field = TagField(\"categories\")\n",
    "year_field = TagField(\"year\")\n",
    "\n",
    "# Create the index with the schema and over documents containing the prefix \"paper:\"\n",
    "await search_index.create(\n",
    "    categories_field,\n",
    "    year_field,\n",
    "    vector_field,\n",
    "    prefix=\"paper:\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test queries!\n",
    "\n",
    "Use the [`running_queries.ipynb`](running_queries.ipynb) notebook to test out queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insurance Claims Example\n",
    "\n",
    "The above code works for the demo dataset or arXiv papers! Extending the example to another use case, we can see how this might work for insurance claims and policy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First delete the other dataset from Redis...\n",
    "await search_index.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claims Schema\n",
    "# Here we assume some insurance claims might have a schema like this:\n",
    "\n",
    "claims_index = SearchIndex(\"claims\", redis_conn)\n",
    "\n",
    "# Sample dummy data\n",
    "claims = [{\n",
    "    \"claims_id\": \"1235\",\n",
    "    \"customer_id\": \"5341345\",\n",
    "    \"timestamp_of_incident\": 1665765963, # Epoch timestamp\n",
    "    \"timestamp_of_submission\": 1666716363,\n",
    "    \"claim_description\": \"This includes written text that describes the incident from the customer's POV\",\n",
    "    \"text_vector\": np.random.random(size=786),\n",
    "    \"age_of_customer\": 33\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To work with this data in RediSearch, we need to construct a schema and create an index\n",
    "\n",
    "# Schema Definitions\n",
    "customer = TagField(\"customer_id\") # to be able to filter/sort by customer ID\n",
    "timestamp_of_incident = NumericField(\"timestamp_of_incident\")\n",
    "timestamp_of_submission = NumericField(\"timestamp_of_submission\")\n",
    "claim_description = TextField(\"claim_description\")\n",
    "text_vector = VectorField(\n",
    "    \"text_vector\",\n",
    "    \"FLAT\", {\n",
    "        \"TYPE\": \"FLOAT32\",\n",
    "        \"DIM\": 768,\n",
    "        \"DISTANCE_METRIC\": \"IP\",\n",
    "        \"INITIAL_CAP\": len(claims),\n",
    "        \"BLOCK_SIZE\": len(claims)\n",
    "    }\n",
    ")\n",
    "age_of_customer = NumericField(\"age_of_customer\")\n",
    "\n",
    "\n",
    "# Create Index\n",
    "await claims_index.create(\n",
    "    customer,\n",
    "    timestamp_of_incident,\n",
    "    timestamp_of_submission,\n",
    "    claim_description,\n",
    "    age_of_customer,\n",
    "    text_vector,\n",
    "    prefix=\"claim:\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to above, we also need to load data to Redis\n",
    "\n",
    "def claim_key(claim_id: str) -> str:\n",
    "    return f\"claim:{claim_id}\"\n",
    "\n",
    "for claim in claims:\n",
    "    claim[\"text_vector\"] = np.asarray(claim[\"text_vector\"], dtype=np.float32).tobytes()\n",
    "    await redis_conn.hset(claim_key(claim[\"claims_id\"]), mapping=claim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "await redis_conn.hgetall(f\"claim:{claim['claims_id']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1c5a7c9cc0d58080444e081b74a0823c09a12f0209aca730c38726ea6940124"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
